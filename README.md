# w2b - Predicting Neural Activity with Language Models: Incorporating Listening Comprehension Strategies

The objective of this project is to explore the viability of employing the GPT-2 transformer-based
language model as a cognitive model for language processing in the human brain. In case it falls short
of being a suitable cognitive model, we aim to identify potential approaches to bridge the gap. To
achieve this aim, functional magnetic resonance imaging (fMRI) scans will be utilized to measure neural
activity in participants as they listen to stories. Subsequently, the neural activity will be compared to
the word embeddings generated by GPT-2.
There are some cognitive tactics that had been previously presented as ones that take place during
listening comprehension. These cognitive tactics can be used as tools for manipulating stimuli
embeddings created by LMs to predict brain responses recorded with fMRI to the best
of our ability.

We utilized
a baseline approach where each word was embedded using the combined embedding of a window
preceding the word. Additionally, we proposed two methods to incorporate cognitive tactics into the
input in an effort to improve the correlation to brain activity. The first method involved adding a
cumulative summary to the input before creating the embeddings, while the second method entailed
adding a prediction of the text continuation after the original text before creating the embeddings.
Our findings revealed that both methods partially preserved the correlations observed in the baseline approach. However, despite these efforts, none of the methods resulted in a significant improvement
in the correlation to neural activity compared to the baseline. Our results highlight the challenges of
predicting brain activity using language model embeddings and the limitations of incorporating cognitive tactics into the input to enhance predictive accuracy.


![width=50](https://user-images.githubusercontent.com/81311717/231746124-084ae71d-b818-407a-a335-46eea15a119b.png)
